{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukumarn/DataVisualization/blob/master/colab_2_prompt_engineering_guide_for_building_llm_applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P645ZVgcjelQ",
        "outputId": "f1cafc3d-1b58-47a7-b1cd-48db3acd05bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.66-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.66 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.10/dist-packages (0.0.66)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langsmith) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langsmith) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith) (2023.7.22)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.2)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=201203be6b2a974d0d39bf05be1c9a4b1d2f7e248dbd9ec747e6491b17b8cd74\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, emoji, backoff, unstructured\n",
            "Successfully installed backoff-2.2.1 emoji-2.8.0 filetype-1.2.0 langdetect-1.0.9 python-iso639-2023.6.15 python-magic-0.4.27 rapidfuzz-3.5.2 unstructured-0.11.0\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
            "Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=9130904a90721c43d89bf181060fa5b34c8d98a2ec8ca7abf2393e032d555834\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, asgiref, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, kubernetes, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 httptools-0.6.1 humanfriendly-10.0 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.2)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.10)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.0.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n",
            "Collecting squarify\n",
            "  Downloading squarify-0.4.3-py3-none-any.whl (4.3 kB)\n",
            "Installing collected packages: squarify\n",
            "Successfully installed squarify-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install langchain\n",
        "!pip install langsmith\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "!pip install ipywidgets\n",
        "!pip install squarify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "AzgcWV8cjlBh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U46SgJJBjelS"
      },
      "source": [
        "**Presentation:**\n",
        "\n",
        "# Introduction to Prompt Engineering and the ChatGPT API\n",
        "\n",
        "- Prompt basics\n",
        "- Introduction to the ChatGPT API\n",
        "- Prompt engineering guide\n",
        "- Best practices for writing effective prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxYc_igWjelT"
      },
      "source": [
        "# Prompting Basics\n",
        "\n",
        "A prompt is a piece of text that conveys to a LLM what the user wants. What the user wants can be many things like:\n",
        "\n",
        "- Asking a question\n",
        "- Giving an instruction\n",
        "- Etc...\n",
        "\n",
        "The key components of a prompt are:\n",
        "1. Task description: where you describe what you want\n",
        "2. Input data: data the model has not seem to illustrate what you need\n",
        "3. Context information: background info on what you are requesting, the data you are providing etc...\n",
        "4. Prompt style: its how you ask the thing to the model and that can greatly influence its performance, for example asking the model [\"Let's think step by step\" can boost reasoning performance](https://arxiv.org/pdf/2201.11903.pdf).\n",
        "\n",
        "[Prompts can also be seen as a form of programming that can customize the outputs and interactions with an LLM.](https://ar5iv.labs.arxiv.org/html/2302.11382#:~:text=prompts%20are%20also%20a%20form%20of%20programming%20that%20can%20customize%20the%20outputs%20and%20interactions%20with%20an%20llm.)\n",
        "\n",
        "One way I like to think about prompts, is as tools that rearrange the weights (probabilities) in the LLM text representation space, to allow you access to a particular sub-universe within the embedding space of the LLM.\n",
        "\n",
        "<span style=\"color: red\">Reconsider the technicalities of wording it like this </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk73RABtjelT"
      },
      "source": [
        "Task Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wF8aZIwyjelT"
      },
      "outputs": [],
      "source": [
        "task_description = \"I want you to write a one paragraph essay about how to learn using generative artificial intelligence.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfTFWQNkjelT"
      },
      "source": [
        "Input Data & Context Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "stiyLnFajelT"
      },
      "outputs": [],
      "source": [
        "input_data = \"Learning Topic examples: [calculus, derivatives, hypothesis testing, probability distributions]\"\n",
        "\n",
        "context_information = \"I am a student who is trying to learn about the mathematical foundations of AI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-0NLjHIjelT"
      },
      "source": [
        "Prompt Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yqzyeBf2jelT"
      },
      "outputs": [],
      "source": [
        "# How you ask what you want, and the heavily relies on what you want from the model.\n",
        "# Instruction prompt:\n",
        "\n",
        "instruction_prompt = f\"{task_description} {input_data} {context_information}. I want the output to be a set of instructive bullet points:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y0EaaDNejelU",
        "outputId": "9cd079b7-e783-464f-ea13-c587efb3143e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To embark on a journey of learning about the mathematical foundations of AI, utilizing generative artificial intelligence can be immensely helpful. Here are some instructive bullet points to guide you through the process:\\n\\n1. Start by assimilating basic knowledge: Familiarize yourself with essential calculus concepts and principles, such as functions, limits, and derivatives. Establish a solid foundation to build upon.\\n\\n2. Dive into derivatives: Expand your understanding of derivatives, exploring differentiation rules, chain rule, and implicit differentiation. Practice solving derivative problems to gain proficiency.\\n\\n3. Delve into hypothesis testing: Learn about the fundamentals of hypothesis testing, including null and alternative hypotheses, p-values, and statistical significance. Understand how to formulate, conduct, and interpret hypothesis tests in various scenarios.\\n\\n4. Grasp probability distributions: Study different probability distributions, including the normal, binomial, and exponential distributions. Comprehend their characteristics, derive key formulas, and grasp the concept of expected values and variances.\\n\\n5. Leverage generative AI: Harness the power of generative artificial intelligence to enhance your learning experience. Utilize AI-powered tools and platforms to generate and analyze sample data, explore complex mathematical relationships, and simulate scenarios.\\n\\n6. Practice and problem-solving: Apply your knowledge by tackling practice problems and real-world applications. Engage in active problem-solving to reinforce your understanding and develop critical thinking skills.\\n\\n7. Collaborate and seek guidance: Join online communities or forums where you can interact with fellow learners and experts. Share insights, ask questions, and seek guidance to gain different perspectives and deepen your understanding.\\n\\n8. Stay updated and be persistent: AI and mathematics are rapidly evolving fields. Stay up to date with the latest advancements, research papers, and applications. Be persistent in your learning journey, embracing challenges as opportunities for growth.\\n\\nBy employing generative artificial intelligence as a learning tool and following these instructive bullet points, you can effectively navigate and comprehend the mathematical foundations of AI. Good luck on your quest for knowledge!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "    {\"role\": \"user\", \"content\": instruction_prompt}\n",
        "])\n",
        "response.choices[0].message.content\n",
        "# prompt = \"What is the meaning of life?\"\n",
        "\n",
        "# client = OpenAI()\n",
        "# client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "#                              messages=\n",
        "#                              [\n",
        "#                                  {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "#                                  {\"role\": \"user\", \"content\": prompt}\n",
        "#                              ],\n",
        "#                              max_tokens=100,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFUhGl34jelV"
      },
      "source": [
        "## Introduction to ChatGPT API\n",
        "\n",
        "- Where does ChatGPT fit into this chaotic universe?\n",
        "- The ChatGPT API (what’s the deal?)\n",
        "- How to use it, basics, parameters, simple examples, etc…."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xi_O8U_jelV"
      },
      "source": [
        "The request body for the CHATGPT API involves many parameters, but let's focus on the following:\n",
        "\n",
        "- model: ID of the model to use.\n",
        "- messages: a list of messages comprising the conversation up to that point\n",
        "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
        "- n: number of chat completion choices to generate for each input message\n",
        "- max_tokens: the maximum number of tokens to generate in the chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2o6iJXsjelV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# load_dotenv(dotenv_path=\"/full/path/to/your/.env\")\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "prompt = \"What is the meaning of life?\"\n",
        "\n",
        "client = OpenAI()\n",
        "response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnvXiXFNjelV"
      },
      "outputs": [],
      "source": [
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yC8hUXijelV"
      },
      "outputs": [],
      "source": [
        "print(\"Response ID:\", response.id)\n",
        "print(\"Choices:\", response.choices)\n",
        "print(\"Created:\", response.created)\n",
        "print(\"Model:\", response.model)\n",
        "print(\"System Fingerprint:\", response.system_fingerprint)\n",
        "print(\"Object Type:\", response.object)\n",
        "print(\"Usage:\", response.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-B_4EBjelV"
      },
      "source": [
        "source: https://platform.openai.com/docs/api-reference/chat/object\n",
        "\n",
        "\n",
        "### The chat completion object\n",
        "\n",
        "Represents a chat completion response returned by model, based on the provided input.\n",
        "\n",
        "__id__\n",
        "- string\n",
        "- A unique identifier for the chat completion.\n",
        "\n",
        "__choices__\n",
        "- array\n",
        "- A list of chat completion choices. Can be more than one if n is greater than 1.\n",
        "\n",
        "__created__\n",
        "- integer\n",
        "- The Unix timestamp (in seconds) of when the chat completion was created.\n",
        "\n",
        "__model__\n",
        "- string\n",
        "- The model used for the chat completion.\n",
        "\n",
        "__system_fingerprint__\n",
        "- string\n",
        "- This fingerprint represents the backend configuration that the model runs with.\n",
        "\n",
        "- Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.\n",
        "\n",
        "__object__\n",
        "- string\n",
        "- The object type, which is always chat.completion.\n",
        "\n",
        "__usage__\n",
        "- object\n",
        "- Usage statistics for the completion request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DgXqzEtjelW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def llm_model(prompt, system_message=\"You are a savy guru with knowledge about existence and the secrets of life.\"):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": system_message},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Tell me a joke as a haiku about Pandas\"\n",
        "response = llm_model(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47gfoONWjelW"
      },
      "source": [
        "# Prompt Engineering Guide\n",
        "\n",
        "What is prompt engineering?\n",
        "\n",
        "Prompt engineering is a reference to a discipline concerned with stablishing the rules for obtaining the most deterministic outputs possible from a LLM by employing engineering techniques and protocols to enture reproducibility and consistency.\n",
        "\n",
        "***In a simplified way, prompt engineering is the means by which LLMs can be programmed through prompting.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rHNhlg2jelW"
      },
      "source": [
        "The basic goal of prompt engineering is designing appropriate inputs for prompting methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dat6FE3wjelW"
      },
      "source": [
        "# Prompt Engineering Techniques\n",
        "\n",
        "Now, let's walk through a simplified guide of prompt engineering techniques:\n",
        "\n",
        "- [Zero-shot Prompting](https://www.promptingguide.ai/techniques/zeroshot#:~:text=Large%20LLMs%20today,examples%20we%20used%3A)\n",
        "- [Few-shot Prompting](https://www.promptingguide.ai/techniques/fewshot#:~:text=few-shot%20prompting%20can%20be%20used%20as%20a%20technique%20to%20enable%20in-context%20learning%20where%20we%20provide%20demonstrations%20in%20the%20prompt%20to%20steer%20the%20model%20to%20better%20performance)\n",
        "- [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot#:~:text=introduced%20in%20wei%20et%20al.%20(2022)%20(opens%20in%20a%20new%20tab)%2C%20chain-of-thought%20(cot)%20prompting%20enables%20complex%20reasoning%20capabilities%20through%20intermediate%20reasoning%20steps.%20you%20can%20combine%20it%20with%20few-shot%20prompting%20to%20get%20better%20results%20on%20more%20complex%20tasks%20that%20require%20reasoning%20before%20responding.)\n",
        "- [Self-consistency](https://www.promptingguide.ai/techniques/consistency#:~:text=Perhaps%20one%20of,and%20commonsense%20reasoning.)\n",
        "- [Generate Knowledge](https://www.promptingguide.ai/techniques/knowledge#:~:text=LLMs%20continue%20to,as%20commonsense%20reasoning%3F)\n",
        "- [Tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot#:~:text=For%20complex%20tasks,with%20language%20models.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VqeKL5EjelW"
      },
      "source": [
        "# Zero-shot Prompting\n",
        "\n",
        "[Zero-shot prompting](https://arxiv.org/pdf/2109.01652.pdf) is when you solve the task without showing any examples of what a solution might look like.\n",
        "\n",
        "For example consider a prompt like:\n",
        "\n",
        "```\n",
        "Classify the sentiment in this sentence as negative or positive:\n",
        "Text: I will go to a vacation\n",
        "Sentiment:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOgjQ7SdjelW"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt = \"\"\"Classify the sentiment in this sentence as negative or positive:\n",
        "Text: I don't like studying at all!.\n",
        "Sentiment:\"\"\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbKOCfROjelW"
      },
      "source": [
        "We can do a few more like:\n",
        "\n",
        "```\n",
        "What is the capital of Canada?\n",
        "Answer:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St8NWEYmjelW"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of Canada?\\nAnswer (one word):\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwXpRNPsjelX"
      },
      "source": [
        "and so on and so forth, one can use this as the first try at a model to see what kinds of tasks that LLM can already solve out of the box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw1EqK90jelX"
      },
      "source": [
        "# Few-shot Prompting\n",
        "\n",
        "As the complexity of a task increases, you might need to provide information in the form of examples to the LLM.\n",
        "\n",
        "**Few-shot Prompting** is a prompting technique where you show a few examples of what a solution might look like.\n",
        "\n",
        "THe goal is to enable what is called 'in-context learning' where the model improves by learning contextual information about the task at hand.\n",
        "\n",
        "We do that by giving demonstrations that will serve as conditionning for subsequent examples where we would like the model to generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM_bpMjSjelX"
      },
      "outputs": [],
      "source": [
        "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt = \"\"\"\n",
        "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
        "the word whatpu is:\n",
        "We were traveling in Africa and we saw these very cute whatpus.\n",
        "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
        "the word farduddle is:\n",
        "\"\"\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvN8CZ8OjelX"
      },
      "outputs": [],
      "source": [
        "# Survey question!\n",
        "few_shot_prompt = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBasDjMIjelX"
      },
      "source": [
        "# Chain-of-Thought\n",
        "\n",
        "This is a prompting technique where we induce step-by-step reasoning and planning within the prompt to enhance performance of the model.\n",
        "\n",
        "According to [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ikx194jelX"
      },
      "outputs": [],
      "source": [
        "# the example was taken from here: https://www.promptingguide.ai/techniques/fewshot\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "prompt = \"\"\"\n",
        "Q: I have one sister and one brother. I am 20 years of age. My sister is 5 years older and my brother 2 years younger than my sister.\n",
        "How old is my brother?\n",
        "A: If I am 20 years of age and my sister is 5 years older, my sister is 20+5=25 years old. If my brother is 2 years younger than my sister, my brother is 25-2=23 years old. The answer is 23 years old.\n",
        "\n",
        "Q: I have 2 friends, Jack and Sally. Jack is 2 years older than Sally. Sally is 5 years younger than me. I am 17 years old. How old is Jack?\n",
        "A:\n",
        "\"\"\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwcoQF1ajelX"
      },
      "outputs": [],
      "source": [
        "# Survey prompt!\n",
        "prompt_CoT = \"?\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOih4UysjelX"
      },
      "source": [
        "You can combine few-shot prompting with chain-of-thought to get better results on highly complex tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2L2IP_pjelX"
      },
      "outputs": [],
      "source": [
        "# source: https://www.promptingguide.ai/techniques/cot\n",
        "prompt = \"\"\"\n",
        "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "Q:The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
        "\n",
        "Q:The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
        "\n",
        "Q:The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
        "Q:The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UxcmSOMjelY"
      },
      "source": [
        "# Self-consistency\n",
        "\n",
        "You use few shot prompting and chain of thoughts to sample a bunch of reasoning paths and then use generations to select the most consistent answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T29xVCz4jelY"
      },
      "outputs": [],
      "source": [
        "# source: https://arxiv.org/pdf/2203.11171.pdf\n",
        "few_shot_CoT_prompt = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "n_reasoning_paths = 5\n",
        "answers = []\n",
        "for i in range(n_reasoning_paths):\n",
        "    response = get_response(few_shot_CoT_prompt)\n",
        "    answers.append(response)\n",
        "    print(response)\n",
        "    print(\"*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jausvpXWjelY"
      },
      "source": [
        "# Generate Knowledge\n",
        "\n",
        "This technique is about inserting knowledge into the prompt in order to yield better performance, you use the model to generate knowledge about a field, and then use that generated knowledge to improve its performance on a downstream task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaDTXaOLjelY"
      },
      "outputs": [],
      "source": [
        "# source: https://www.promptingguide.ai/techniques/knowledge\n",
        "prompt = \"\"\"Input: Greece is larger than mexico.\n",
        "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
        "Input: Glasses always fog up.\n",
        "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
        "Input: A fish is capable of thinking.\n",
        "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
        "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
        "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
        "Input: A rock is the same size as a pebble.\n",
        "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
        "Input: Part of golf is trying to get a higher point total than others.\n",
        "Knowledge:\"\"\"\n",
        "knowledges = []\n",
        "num_knowledges = 3\n",
        "for i in range(num_knowledges):\n",
        "    knowledges.append(get_response(prompt))\n",
        "\n",
        "print(knowledges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8ZngGvjelY"
      },
      "source": [
        "We integrate the knowledge to get a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X9TGTrnjelY"
      },
      "outputs": [],
      "source": [
        "# source: https://www.promptingguide.ai/techniques/knowledge\n",
        "prompt = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
        "Explain and Answer: \"\"\"\n",
        "\n",
        "get_response(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoVQkPHKjeli"
      },
      "source": [
        "# Tree of thoughts (ToT)\n",
        "\n",
        "\n",
        "ToT [Long (2023)](https://arxiv.org/pdf/2305.08291.pdf) is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that ser as intermediate steps for general problem solving with LMs.\n",
        "\n",
        "This technique involves a framework where a tree of thoughts is maintained, where a thought here means a coherent sequence of steps that represent moving forward in the solution. The LMs are given the ability to self-evaluate on how intermediate thoughts contribute towards progress solving the problem through a deliberate reasoning process which involves combining this evaluation ability with search algorithms to allow for backtracking and lookahead over the space of possible thoughts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ZFAvNLjeli"
      },
      "source": [
        "![](https://github.com/EnkrateiaLucca/oreilly_live_training_llm_apps/blob/main/notebooks/images/ToT_framework.png?raw=1)\n",
        "Image Source: [Yao et al. (2023)](https://arxiv.org/pdf/2305.08291.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vts1Bimcjeli"
      },
      "source": [
        "# Many More but That's Enough\n",
        "\n",
        "There are many more prompt engineering techniques that grow in complexity like:\n",
        "- [Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)\n",
        "- [Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n",
        "- [Active Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n",
        "- [Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n",
        "- [React Prompting](https://www.promptingguide.ai/techniques/react)\n",
        "- [Mulitmodal CoT](https://www.promptingguide.ai/techniques/multimodalcot)\n",
        "- [Graph Prompting](https://www.promptingguide.ai/techniques/graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7MXIxHijeli"
      },
      "source": [
        "# Prompt Engineering Practical Case Study\n",
        "\n",
        "Now, let's take the concepts and ideas discussed in this lesson, and apply them to an actual problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMpuFuSujeli"
      },
      "source": [
        "Let's start with a simple example, imagine you want to extract dates from text. You might set up a LLM to do that by first creating a set of examples of phrases with dates, something we can start with ChatGPT itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3TNEWrHjeli"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "num_samples = 10\n",
        "phrases_with_dates = []\n",
        "prompt = \"Create a 1 paragraph phrase containing a complete date (day month  and year) anywhere in the text formatted in different ways.\"\n",
        "for i in range(num_samples):\n",
        "    phrases_with_dates.append(get_response(prompt))\n",
        "phrases_with_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XUOY3UPjeli"
      },
      "source": [
        "Ok perfect! Now that we have this evaluation set, we can set up a simple experiment by first creating a demonstration set with our prompt candidates.\n",
        "\n",
        "We'll begin with a baseline using only zero-shot prompt examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRTzbjpxjeli"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompts = [\"Extract the date from this text as DD-MM-YYYY\",\n",
        "                     \"Fetch the date from this text as DD-MM-YYYY\",\n",
        "                     \"Get the date from this phrase as DD-MM-YYYY\",\n",
        "                     \"Below is a text containing a date. Extract that date in the format: <DD-MM-YYYY>\"\n",
        "                     ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQjuqpapjelj"
      },
      "source": [
        "Ok, we have our candidates, so let's now test them creating a table with the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8v8zcGIjelj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "for phrase in phrases_with_dates:\n",
        "    for prompt in zero_shot_prompts:\n",
        "        response = get_response(prompt + \" \" + phrase)\n",
        "        data.append([phrase, prompt, response])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=data, columns=['phrase','prompt', 'response'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-yOoqrfjelj"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "# parse a text response to extract a date formatted as DD-MM-YYYY\n",
        "def extract_date(text):\n",
        "    \"\"\"Date parser\"\"\"\n",
        "    # regex pattern for date\n",
        "    date_pattern = r\"(\\d{1,2})-(\\d{1,2})-(\\d{4})\"\n",
        "    # extract date from text\n",
        "    date = re.search(date_pattern, text)\n",
        "    # return date\n",
        "    return date.group(0) if date else None\n",
        "\n",
        "# apply the function to the 'response' column of the dataframe df\n",
        "df['date'] = df['response'].apply(extract_date)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_3r-uvbjelj"
      },
      "source": [
        "Ok, now that we have some results for the dates that were parsed, we need a way to measure performance so we can compare how well they did. In this case, we'll consider a point for the score of the prompt if a date was properly extracted after running the `extract_date()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a72JXWRrjelj"
      },
      "outputs": [],
      "source": [
        "# create a column that is 1 if the date value is not None or 0 otherwise\n",
        "df['scores'] = df['date'].apply(lambda x: 1 if x is not None else 0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_beBoRL9jelj"
      },
      "outputs": [],
      "source": [
        "# group by prmopts creating an accuracy column that is the result of summing over the scores and dividing by 20\n",
        "# then sort by accuracy\n",
        "df_performance = df.groupby('prompt').agg({'scores': 'sum'}).sort_values(by='scores', ascending=False)\n",
        "df_performance[\"scores\"] = (df_performance[\"scores\"] / num_samples)*100\n",
        "df_performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78u26SGejelj"
      },
      "source": [
        "The limitations of this example:\n",
        "- Testing more types of prompt candidate categories (like few shot prompting for example)\n",
        "- Enforcing the output size to convert to the date format instead of doing post processing on the output\n",
        "- Better scoring strategy than just None or correct (something that evaluates the outputs semantically for truthfullness)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPeYUIInjelj"
      },
      "source": [
        "Prompt Engineering Simplified Template\n",
        "- Stablish a concrete and atomic task\n",
        "- Define a set of prompt candidates\n",
        "- Define a clear metric for evaluation\n",
        "- Test\n",
        "- Evaluate\n",
        "- Compare\n",
        "- Find the best prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PFmlJdIjelj"
      },
      "source": [
        "Perfect! There we have it, our first results! The way to evolve this approach would be to test on a harder test set and if we don't get good results, we try better prompting strategies like few-shot, self-consistency, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lY36Ji2jelj"
      },
      "source": [
        "# A Slightly More Complex Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW6iGKdHjelj"
      },
      "source": [
        "In this example we'll look at desgining a simple prompt engineering experiment to find the best prompt to generate an intuitive and simple explanation of a concept.\n",
        "\n",
        "The idea is that, given a concept, or piece of information we would like to understand, the model should output a simple one paragraph explanation giving all the necessary context and information to allow the user to grasp the concept at hand.\n",
        "\n",
        "Let's start by creating a few prompt candidates, in the beggining its always a good idea to come up with a few prompts yourself, and preferably zero-shot examples which would be the baseline upon which we'll improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5vLWnBRjelj"
      },
      "outputs": [],
      "source": [
        "prompt_candidates = [\"Explain this concept in simple terms\",\n",
        "                     \"Explain the following concept:\",\n",
        "                     \"Explain this:\",\n",
        "                     \"Break down this concept for a beginner:\",\n",
        "                    \"Can you simplify the explanation of the following concept:\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Qn5SAijelj"
      },
      "source": [
        "Ok, now that we have our candidates, let's run a first experiment. Given the subjective and general nature of the problems dealt by LLMs, its hard to settle on one precise metrics as we would in supervised learning scenarios.\n",
        "\n",
        "Therefore, what we would like to do is to use GPT-4 as the judge for the quality of our models, this approach is actually a common place in prompt engineering papers, and its one that yields some quite impressive results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x4vLrI8jelj"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def gpt4_score(response, concept):\n",
        "    score_prompt = f\"Give a score from 0 to 100 to this response: {response} based on how well it represents an explanation of this concept: {concept} \"\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are an expert tutor in all scientific fields.\"},\n",
        "                                 {\"role\": \"user\", \"content\": score_prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.0,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "def get_response(prompt):\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
        "                             messages=\n",
        "                             [\n",
        "                                 {\"role\": \"system\", \"content\": \"You are a savy guru with knowledge about existence and the secrets of life.\"},\n",
        "                                 {\"role\": \"user\", \"content\": prompt}\n",
        "                             ],\n",
        "                             max_tokens=100,\n",
        "                             temperature=0.9,\n",
        "                             n = 1)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "data = []\n",
        "concept_list = [\"Genetic Mutations\",\n",
        "                \"Overfitting in Machine Learning\",]\n",
        "\n",
        "\n",
        "\n",
        "for concept in concept_list:\n",
        "    for prompt in prompt_candidates:\n",
        "        response = get_response(prompt + \" \" + concept)\n",
        "        response_score = gpt4_score(response, concept)\n",
        "        data.append([prompt, response, response_score, concept])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"prompt\", \"response\", \"response_score\", \"concept\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6-l-_-Fjelk"
      },
      "outputs": [],
      "source": [
        "df.to_csv('prompt_engineering_results2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZf0We4Hjelk"
      },
      "source": [
        "Perfect! We can see that the score given by the model needs some cleaning up (this is actually an issue that wil be solved by a tool we'll introduce in the next section), so let's do that quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFPuV4oGjelk"
      },
      "outputs": [],
      "source": [
        "for i,score_output in enumerate(df[\"response_score\"]):\n",
        "    score_parsed = f\"Given this response, extract the score value and return only that: {score_output}. NUMBER ONLY.\"\n",
        "    score_parsed = get_response(score_parsed)\n",
        "    # replace the response score row with this newly parsed score value\n",
        "    df.loc[i,\"response_score\"] = score_parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp1Ngy0mjelk"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alSWJSn6jelk"
      },
      "source": [
        "Let's take a look at the results so far:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSnb7EHEjelk"
      },
      "source": [
        "Ok, we have some results, now let's take a look at the best performing prompts and compare the answers with the lower performing ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC7_yLO-jelk"
      },
      "outputs": [],
      "source": [
        "# # compare lower the responses from the rows with the highest resopnse_score value and the lowest response_score value\n",
        "# # to see if there is a difference in the responses\n",
        "\n",
        "# #df[df['response_score'] == df['response_score'].max()]['response'].values\n",
        "# df[df.groupby('concept')['response_score'].transform(max) == df['response_score']]['response'].values\n",
        "\n",
        "# #df[df['response_score'] == df['response_score'].min()]['response'].values\n",
        "\n",
        "# Get rows with the best response for each concept\n",
        "best_responses = df[df.groupby('concept')['response_score'].transform(max) == df['response_score']]\n",
        "\n",
        "# Get rows with the worst response for each concept\n",
        "worst_responses = df[df.groupby('concept')['response_score'].transform(min) == df['response_score']]\n",
        "\n",
        "# Iterate over unique concepts and print best and worst responses\n",
        "for concept in df['concept'].unique():\n",
        "    best_response = best_responses[best_responses['concept'] == concept]['response'].values[0]\n",
        "    worst_response = worst_responses[worst_responses['concept'] == concept]['response'].values[0]\n",
        "\n",
        "    print(f\"Concept: {concept}\")\n",
        "    print(f\"Best Response: {best_response}\")\n",
        "    print(f\"------\")\n",
        "    print(f\"Worst Response: {worst_response}\")\n",
        "    print(\"------\")\n",
        "    print(\"*\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJnWqcN-jelk"
      },
      "source": [
        "Usually, you would have to tune even the prompt that is being used to create the scores for the responses, to make sure you have the best possible results, but for this particular case let's just analyse overall how well we did with these baseline preliminary results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcc6P4cxjelk"
      },
      "source": [
        "# References\n",
        "\n",
        "- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://ar5iv.labs.arxiv.org/html/2302.11382)\n",
        "- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)\n",
        "- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
        "- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)\n",
        "- [prompt engineering guide - zero shot prompting example](https://www.promptingguide.ai/techniques/zeroshot)\n",
        "- [Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)\n",
        "- [prompt engineering guide - few shot prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
        "- [prompt engineering guide - chain of thought prompting](https://www.promptingguide.ai/techniques/cot)\n",
        "- [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n",
        "- [prompt engineering guide - self-consistency](https://www.promptingguide.ai/techniques/consistency)\n",
        "- [prompt engineering guide - generate knowledge](https://www.promptingguide.ai/techniques/knowledge)\n",
        "- [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)\n",
        "- [prompt engineering guide - tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n",
        "- [Prompt Engineering by Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
        "- [Prompt Engineering vs. Blind Prompting](https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting#the-demonstration-set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGkK-HX0GpZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oreilly_env",
      "language": "python",
      "name": "oreilly_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}